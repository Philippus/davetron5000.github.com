
<!DOCTYPE html>
<html language="en"><head>
  <meta charset="utf-8">
  <title>Simple Metrics for Team and Process Improvement - naildrivin5.com - David Bryant Copeland's Website</title>
  <meta name="author" content="David Bryant Copeland">
  <link rel="apple-touch-icon" sizes="57x57" href="/images/favicons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/images/favicons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/images/favicons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/images/favicons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/images/favicons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/images/favicons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/images/favicons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/images/favicons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/images/favicons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/images/favicons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/images/favicons/ms-icon-144x144.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  

  
  <meta name="description" content="

  
    Simple Metrics for Team and Process Improvement
    
      June 29, 2009
    
  
  
    Recently, the development team where I work has st...">
  

  
  <link rel="canonical" href="http://naildrivin5.com/blog/2009/06/29/simple-metrics-for-team-and-process-improvement.html">
  <link href="/favicon.png" rel="icon">
  <link href="/atom.xml" rel="alternate" title="naildrivin5.com - David Bryant Copeland's Website" type="application/atom+xml">
  <link href="/css/styles.css" rel="stylesheet">
  <script src="https://use.typekit.net/ylm4zpa.js"></script>
  <script>try{Typekit.load({ async: true });}catch(e){}</script>
            
  <meta name="google-site-verification" content="h_yTpXa6N3ebHj8DYmgX4lIFGHBW1NtGMVHfXuu7i_4" />
</head>
<body>
  <header class="site-header pb2 mb2 center">
    <h1 class="uc ls2 f1"><a href="/">naildrivin5.com</a></h1>
    <h2 class="ls1"><small class="uc h4">Website of</small> David Bryant Copeland</h2>
    <nav>
      <ul>
        <li class="text-c w-1-3"><a class="ib w-100 button" href="/">Blog</a></li>
        <li class="text-c w-1-3"><a class="ib w-100 button" href="/bio">About</a></li>
        <li class="text-c w-1-3"><a class="ib w-100 button" href="/books">Books</a></li>
        <li class="text-c w-1-3"><a class="ib w-100 button" href="/talks">Talks</a></li>
      </ul>
      <div style="clear: both"></div>
    </nav>
  </header>
  <main>
    <section>
      <div>
<article role="article" class="blog-post">
  <header class="border-bottom border-light">
    <h1>Simple Metrics for Team and Process Improvement</h1>
    <h2 class="f5 ls2 fw-bold mtnone uc ib">
      June 29, 2009
    </h2>
  </header>
  <section class="blog-content">
    <p>Recently, the development team where I work has started collecting bona-fide metrics, based on our ticketing system.  So few development shops (especially small ones) collect real information on how they work that it's exciting that we're doing it.
<p>
<p>
Here's what we're doing:
<ul>
<li><b>Number of releases during QA</b> (we do a daily release, so more than daily is an indicator)</li>
<li><b>Defects found, by severity and priority</b></li>
<li><b>Average time from accepting a ticket (starting work) to resolving it (sending it for testing)</b></li>
<li><b>Number of re-opens</b> (i.e. a defect was sent to testing, but not fixed)</li>
<li><b>Average time from resolving to closing</b> (i.e. testing the fix)</li>
<li><b>Defects due to coding errors vs. unclear requirements</b> (this is really great to be able to collect; with our company so new and small, we can introduce this and <b>use</b> it without ruffling a lot of feathers)</li>
</ul>
</p>
<p>
The tricky thing about metrics is that they are not terribly meaningful by themselves; rather they indicate areas for focussed investigation.  For example, if it takes an average of 1 day to resolve a ticket, but 3 days to test and close it, we don't just conclude that testing is inefficient; we have to investigate why.  Perhaps we don't have enough testers.  Perhaps our testing environment isn't stable enough.  Perhaps there are too many show-stoppers that put the testers on the bench while developers are fixing them.
</p>
<p>
Another way to interpret these values is to watch them over time.  If the number of critical defects is decreasing, it stands to reason we're doing a good job.  If the number of re-opens is increasing, we are packing too much into one iteration and possibly not doing sufficient requirements analysis.  We <b>just</b> started collecting these on the most recent iteration, so in the coming months, it will be pretty cool to see what happens.
</p>
<p>
These metrics are pretty basic, but it's great to be collecting them.  The one thing that can make hard-core analysis of these numbers (esp. over time as the team grows and new projects are created) is the lack of normalization.  If we introduced twice as many critical bugs this iteration than last, are we necessarily "doing worse"?  What if the requirements were more complex, or the code required was just...bigger?</p>
<p>
Normalizing factors like cyclomatic complexity, lines of code, etc, can shed some more light on these questions.  These normalizing factors aren't always popular, but interpreted the right way, could be very informative.  We're the same team, using the same language, working on the same product.  If iteration 14 adds 400 lines of code, with 3 critical bugs, but iteration 15 adds 800 lines of code with 4 critical bugs, I think we can draw some real conclusions (i.e. we're getting better).  
</p>
<p>
Another interesting bit of data would be to incorporate our weekly code review.  We typically review fresh-but-not-too-fresh code, mostly for knowledge sharing and general "architectural consistency".  If we were to actively review code in development, before it is sent to testing, we could then have real data on the effectiveness of our code reviews.  Are we finding lots of coding errors at testing time?  Maybe more code reviews would help?  Are we finding fewer critical bugs in iteration 25, than in iteration 24 and 23, where we weren't doing reviews?  Reviews helped a lot.
</p>
<p>
These are actually really simple things to do (especially with a small, cohesive team), and can shed real light on the development process.  What else can be done?
</p>

  </section>
</article>
</div>

    </section>
  </main>
  <footer class="center">
    <p class="f6">
      Copyright &copy; 2006-2016, by David Bryant Copeland, All Rights Reserved
    </p>
  </footer>
  <script type="text/javascript">
    var _gauges = _gauges || [];
    (function() {
      var t   = document.createElement('script');
      t.type  = 'text/javascript';
      t.async = true;
      t.id    = 'gauges-tracker';
      t.setAttribute('data-site-id', '56a22cd9c88d90284f000861');
      t.setAttribute('data-track-path', 'https://track.gaug.es/track.gif');
      t.src = 'https://d36ee2fcip1434.cloudfront.net/track.js';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(t, s);
    })();
  </script>
</body></html>
